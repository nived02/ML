# -*- coding: utf-8 -*-
"""EE769  Assignment 1: Exploratory and Descriptive Data Analysis .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MbKmv1T_DJhaLEymimp60jRfR-xvTgN8

# Video Link =

# 1. Check out the City of Los Angeles public data sources and test the hypothesis that the statistics of affordable housing projects” (government housing for low-income people) in a ZIP code has a relation to the health inspection scores of the restaurants in that ZIP code.
"""

import pandas as pd  # Import the pandas library for data manipulation and analysis
import matplotlib.pyplot as plt  # Import matplotlib for creating static, animated, and interactive visualizations
import seaborn as sns  # Import seaborn for making statistical graphics with enhanced visualizations
#[1][2][3]

# Load datasets
inspection_data = pd.read_csv('restaurant_and_market_health_inspections.csv')  # Load health inspection data from a CSV file
housing_data = pd.read_csv('hcidla_affordable_housing_projects.csv')  # Load affordable housing data from a CSV file



"""# **INSPECTION DATA EDA**"""

# Check Data Types
print(inspection_data.dtypes)  # Display the data types of each column in the inspection_data DataFrame

inspection_data['facility_zip'] = inspection_data['facility_zip'].astype(str)  # Ensure it's a string
inspection_data['facility_zip'] = inspection_data['facility_zip'].str.replace('-', '', regex=False)  # Remove hyphens
inspection_data['facility_zip'] = inspection_data['facility_zip'].str[:5]  # Keep only the first 5 digits

# Convert data types
inspection_data['activity_date'] = pd.to_datetime(inspection_data['activity_date'])  # Convert the 'activity_date' column to datetime format
inspection_data['facility_zip'] = inspection_data['facility_zip'].astype(int)  # Convert the 'facility_zip' column to integer type

print(inspection_data.dtypes)

# Analyze unique values in specific columns
print(inspection_data['facility_zip'].nunique())  # Print the number of unique ZIP codes in the 'facility_zip' column
print(inspection_data['facility_name'].nunique())  # Print the number of unique facility names in the 'facility_name' column
print(inspection_data['grade'].unique())  # Print the unique values in the 'grade' column

# Clean and handle missing data in the 'grade' column
inspection_data['grade'].replace(' ', 'Ungraded', inplace=True)  # Replace empty spaces in the 'grade' column with 'Ungraded'
inspection_data['grade'].fillna('Ungraded', inplace=True)  # Fill any NaN values in the 'grade' column with 'Ungraded'
print(inspection_data['grade'].unique())  # Print the unique values in the 'grade' column after cleaning

# Visualize the distribution of health inspection grades
plt.figure(figsize=(8, 6))  # Create a new figure with a specified size
sns.countplot(x='grade', data=inspection_data, palette='pastel', order=['A', 'B', 'C', 'Ungraded'])  # Create a count plot of inspection grades with a pastel color palette and a specific order
plt.title('Distribution of Health Inspection Grades')  # Set the title of the plot
plt.xlabel('Grade')  # Label the x-axis as 'Grade'
plt.ylabel('Number of Inspections')  # Label the y-axis as 'Number of Inspections'
plt.show()  # Display the plot
#[2]

# Display unique values in the 'score' column
print(inspection_data['score'].unique())  # Print the unique values in the 'score' column of the inspection_data DataFrame

# Count and display the most frequent ZIP codes
zip_code_counts = inspection_data['facility_zip'].value_counts()  # Count occurrences of each ZIP code in the 'facility_zip' column
print(zip_code_counts.head(10))  # Print the top 10 ZIP codes with the highest number of inspections

plt.figure(figsize=(12, 8))  # Create a new figure with a specified size
sns.barplot(x=zip_code_counts.index[:20], y=zip_code_counts.values[:20], palette='viridis', hue=zip_code_counts.index[:20], dodge=False)  # Create a bar plot for the top 20 ZIP codes by number of inspections using the 'viridis' color palette
plt.title('Top 20 ZIP Codes by Number of Inspections')  # Set the title of the plot
plt.xlabel('ZIP Code')  # Label the x-axis as 'ZIP Code'
plt.ylabel('Number of Inspections')  # Label the y-axis as 'Number of Inspections'
plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees for better readability
plt.legend([],[], frameon=False)  # Remove the legend to avoid unnecessary space if not needed
plt.show()  # Display the plot
#[2]

# Calculate average inspection scores by ZIP code
avg_scores_by_zip = inspection_data.groupby('facility_zip')['score'].mean().sort_values(ascending=False)  # Group by 'facility_zip', calculate mean 'score' for each ZIP code, and sort in descending order
print(avg_scores_by_zip.head(30))  # Display the top 30 ZIP codes with the highest average scores

# Visualize the top 20 ZIP codes by average inspection score
plt.figure(figsize=(12, 8))  # Create a new figure with a specified size
sns.barplot(x=avg_scores_by_zip.index[:20], y=avg_scores_by_zip.values[:20], palette='viridis')  # Create a bar plot for the top 20 ZIP codes by average inspection score using the 'viridis' color palette
plt.title('Top 20 ZIP Codes by Average Inspection Score')  # Set the title of the plot
plt.xlabel('ZIP Code')  # Label the x-axis as 'ZIP Code'
plt.ylabel('Average Score')  # Label the y-axis as 'Average Score'
plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees for better readability
plt.show()  # Display the plot

# Visualize the distribution of health inspection scores
plt.figure(figsize=(8, 6))  # Create a new figure with a specified size
sns.histplot(inspection_data['score'], bins=20, kde=True, color='blue')  # Create a histogram of the 'score' column with 20 bins, including a KDE (Kernel Density Estimate) curve, and color the plot blue
plt.title('Distribution of Health Inspection Scores')  # Set the title of the plot
plt.xlabel('Score')  # Label the x-axis as 'Score'
plt.ylabel('Frequency')  # Label the y-axis as 'Frequency'
plt.show()  # Display the plot

# Display the count of missing values for each column
print(inspection_data.isnull().sum())  # Print the number of missing (NaN) values for each column in the inspection_data DataFrame

# Remove rows with missing values in specified columns
inspection_data.dropna(subset=['pe_description', 'program_element_pe', 'program_name', 'program_status', 'record_id'], inplace=True)  # Drop rows where any of the specified columns have NaN values and modify the DataFrame in place

print(inspection_data.isnull().sum())  # Print the number of missing (NaN) values for each column in the inspection_data DataFrame after handling missing values

"""Checking whether the 2 columns namely program_name and facility_name is exactly same or not. If same , we can delete any one of them"""

# Create a DataFrame to compare the 'program_name' and 'facility_name' columns
comparison_df = inspection_data[['program_name', 'facility_name']].drop_duplicates()  # Create a new DataFrame with unique rows based on 'program_name' and 'facility_name'
comparison_df['match'] = comparison_df['program_name'].str.strip().str.lower() == comparison_df['facility_name'].str.strip().str.lower()  # Compare the two columns after stripping whitespace and converting to lowercase, storing the result as a boolean in 'match'

# Display rows where there is a mismatch
mismatches = comparison_df[~comparison_df['match']]  # Filter rows where 'match' is False
print(f"Number of mismatches: {len(mismatches)}")  # Print the number of mismatched rows
print(mismatches.head(10))  # Print the first 10 rows of mismatches
#[1][6]

"""# **Housing data EDA**"""

# Check the data types of each column in the housing_data DataFrame
print(housing_data.dtypes)  # Display the data types for each column in the housing_data DataFrame

# Convert columns to datetime format
housing_data['DATE FUNDED'] = pd.to_datetime(housing_data['DATE FUNDED'], errors='coerce')  # Convert the 'DATE FUNDED' column to datetime format, setting invalid parsing to NaT
housing_data['IN-SERVICE DATE'] = pd.to_datetime(housing_data['IN-SERVICE DATE'], errors='coerce')  # Convert the 'IN-SERVICE DATE' column to datetime format, setting invalid parsing to NaT

# Handle missing values and convert data type
housing_data['JOBS'] = housing_data['JOBS'].fillna(0).astype(int)  # Fill missing values in the 'JOBS' column with 0 and convert the column to integer type

# Check the data types of each column in the housing_data DataFrame after conversion
print(housing_data.dtypes)  # Display the updated data types for each column in the housing_data DataFrame

# Display the count of missing values for each column
print(housing_data.isnull().sum())  # Print the number of missing (NaN) values for each column in the housing_data DataFrame

# Clean and preprocess the housing_data DataFrame
housing_data.dropna(subset=['DEVELOPMENT STAGE', 'DATE FUNDED'], inplace=True)  # Remove rows with missing values in 'DEVELOPMENT STAGE' or 'DATE FUNDED' columns
housing_data['CONSTRUCTION TYPE'].fillna('Unknown', inplace=True)  # Fill missing values in 'CONSTRUCTION TYPE' with 'Unknown'
housing_data['SITE COMMUNITY'].fillna('Unknown', inplace=True)  # Fill missing values in 'SITE COMMUNITY' with 'Unknown'
housing_data['HOUSING TYPE'].fillna('Unknown', inplace=True)  # Fill missing values in 'HOUSING TYPE' with 'Unknown'
housing_data['IN-SERVICE DATE'].fillna(pd.Timestamp('1900-01-01'), inplace=True)  # Fill missing values in 'IN-SERVICE DATE' with a placeholder date (January 1, 1900)
housing_data['DEVELOPER'].fillna('Unknown', inplace=True)  # Fill missing values in 'DEVELOPER' with 'Unknown'
housing_data['MANAGEMENT COMPANY'].fillna('Unknown', inplace=True)  # Fill missing values in 'MANAGEMENT COMPANY' with 'Unknown'
housing_data['JOBS'].fillna(0, inplace=True)  # Fill missing values in 'JOBS' with 0
housing_data.drop(columns=['CONTACT PHONE', 'CONTRACT NUMBERS'], inplace=True)  # Remove the 'CONTACT PHONE' and 'CONTRACT NUMBERS' columns from the DataFrame
#[1][6][7]

# Display the count of missing values for each column
print(housing_data.isnull().sum())  # Print the number of missing (NaN) values for each column in the housing_data DataFrame after cleaning

"""After checking the excel file , in housing data the zipcode is mixed  inside the SITE ADDRESS Column . Therfore we need to extract from these numerical values from these columns and make a new column"""

import re

# Function to extract ZIP code from an address
def extract_zip_code(address):
    match = re.search(r'\b\d{5}\b', address)  # Search for a 5-digit ZIP code in the address
    return match.group(0) if match else None  # Return the ZIP code if found, otherwise return None

# Apply the function to the 'SITE ADDRESS' column
housing_data['ZIP_CODE'] = housing_data['SITE ADDRESS'].apply(extract_zip_code)  # Create a new column 'ZIP_CODE' by applying the extract_zip_code function to 'SITE ADDRESS'

housing_data.to_csv('housing_data.csv', index=False)  # Save the updated DataFrame to a CSV file named 'housing_data.csv' without the index

# Check the first few rows to verify
print(housing_data[['SITE ADDRESS', 'ZIP_CODE']].head())  # Print the first few rows of 'SITE ADDRESS' and the newly created 'ZIP_CODE' column to verify the extraction
#[7]

# Display the count of missing values for each column
print(housing_data.isnull().sum())  # Print the number of missing (NaN) values for each column in the housing_data DataFrame after adding the 'ZIP_CODE' column

# Check for missing values and unique values in the 'ZIP_CODE' column
print(housing_data['ZIP_CODE'].isnull().sum())  # Print the number of missing (NaN) values in the 'ZIP_CODE' column
print(housing_data['ZIP_CODE'].unique())  # Print the unique values found in the 'ZIP_CODE' column

# Visualize the top 20 ZIP codes with the most affordable housing projects
zip_code_projects = housing_data['ZIP_CODE'].value_counts()  # Count occurrences of each ZIP code in the 'ZIP_CODE' column

plt.figure(figsize=(12, 8))  # Create a new figure with a specified size
sns.barplot(x=zip_code_projects.index[:20], y=zip_code_projects.values[:20], palette='viridis', hue=zip_code_projects.index[:20], dodge=False)  # Create a bar plot for the top 20 ZIP codes by number of projects using the 'viridis' color palette
plt.title('Top 20 ZIP Codes by Number of Affordable Housing Projects')  # Set the title of the plot
plt.xlabel('ZIP Code')  # Label the x-axis as 'ZIP Code'
plt.ylabel('Number of Projects')  # Label the y-axis as 'Number of Projects'
plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees for better readability
plt.legend([],[], frameon=False)  # Remove the legend to avoid unnecessary space if not needed
plt.show()  # Display the plot
#[2]

# Visualize the distribution of housing types
plt.figure(figsize=(12, 8))  # Create a new figure with a specified size
sns.countplot(x='HOUSING TYPE', data=housing_data, palette='pastel', hue=housing_data['HOUSING TYPE'])  # Create a count plot of 'HOUSING TYPE' with a pastel color palette
plt.title('Distribution of Housing Types')  # Set the title of the plot
plt.xlabel('Housing Type')  # Label the x-axis as 'Housing Type'
plt.ylabel('Number of Projects')  # Label the y-axis as 'Number of Projects'
plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees for better readability
plt.legend([],[], frameon=False)  # Remove the legend to avoid unnecessary space if not needed
plt.show()  # Display the plot
#[2]

# Visualize the distribution of project sizes (total units)
plt.figure(figsize=(8, 6))  # Create a new figure with a specified size
sns.histplot(housing_data['PROJECT TOTAL UNITS'], bins=20, kde=True, color='green')  # Create a histogram of 'PROJECT TOTAL UNITS' with 20 bins, including a KDE (Kernel Density Estimate) curve, and color the plot green
plt.title('Distribution of Project Sizes (Total Units)')  # Set the title of the plot
plt.xlabel('Total Units')  # Label the x-axis as 'Total Units'
plt.ylabel('Frequency')  # Label the y-axis as 'Frequency'
plt.show()  # Display the plot
#[2]

# Print the column names of the housing_data DataFrame
print(housing_data.columns)  # Display the names of all columns in the housing_data DataFrame

# Check for missing values in specific columns
print(housing_data['SITE ADDRESS'].isnull().sum())  # Print the number of missing (NaN) values in the 'SITE ADDRESS' column of the housing_data DataFrame
print(inspection_data['facility_zip'].isnull().sum())  # Print the number of missing (NaN) values in the 'facility_zip' column of the inspection_data DataFrame

"""# c) Summarize each file by ZIP code using SQL: [2]
# i. Ensure the right type of summarization (sum, mean, max etc.) for the other variables
"""

import sqlite3  # Import the SQLite database library for connecting to and interacting with SQLite databases
import pandas as pd  # Import the pandas library for data manipulation and analysis

# Establish a connection to an SQLite database
conn = sqlite3.connect('data_summary.db')  # Connect to the SQLite database named 'data_summary.db'. If the file does not exist, it will be created.

# Load inspection data into SQLite
#inspection_data = pd.read_csv('restaurant_and_market_health_inspections.csv')
inspection_data.to_sql('inspection_data', conn, if_exists='replace', index=False)

# Load housing data into SQLite
#housing_data = pd.read_csv('hcidla_affordable_housing_projects.csv')
housing_data.to_sql('housing_data', conn, if_exists='replace', index=False)

# Retrieve and display the schema of the 'housing_data' table
schema_query = "PRAGMA table_info(housing_data);"  # SQL query to get schema information for the 'housing_data' table
schema = pd.read_sql_query(schema_query, conn)  # Execute the query and load the result into a DataFrame
print(schema)  # Print the DataFrame containing the schema details of the 'housing_data' table
#[6][5]

# Query for inspection data summary
inspection_query = """
SELECT
    facility_zip AS ZIP_CODE,
    COUNT(*) AS total_inspections,  -- Total number of inspections per ZIP code
    AVG(score) AS average_score,    -- Average score per ZIP code
    MAX(score) AS highest_score,    -- Highest score per ZIP code
    MIN(score) AS lowest_score,     -- Lowest score per ZIP code
    COUNT(CASE WHEN grade = 'A' THEN 1 END) AS grade_A_count,  -- Count of 'A' grades per ZIP code
    COUNT(CASE WHEN grade = 'B' THEN 1 END) AS grade_B_count,  -- Count of 'B' grades per ZIP code
    COUNT(CASE WHEN grade = 'C' THEN 1 END) AS grade_C_count,  -- Count of 'C' grades per ZIP code
    COUNT(CASE WHEN grade = ' ' OR grade IS NULL THEN 1 END) AS ungraded_count  -- Count of ungraded inspections per ZIP code
FROM
    inspection_data
GROUP BY
    facility_zip
ORDER BY
    total_inspections DESC;  -- Order results by total number of inspections in descending order
"""

# Execute the query and fetch results
inspection_summary = pd.read_sql_query(inspection_query, conn)  # Run the SQL query and load the result into a DataFrame
print(inspection_summary.head())  # Print the first few rows of the summary DataFrame
#[6]

# Query for housing data summary
housing_query = """
SELECT
    ZIP_CODE,
    COUNT(*) AS total_projects,  -- Total number of housing projects per ZIP code
    SUM(`SITE UNITS`) AS total_units,  -- Total number of units across all projects per ZIP code
    AVG(`SITE UNITS`) AS average_units_per_project,  -- Average number of units per project per ZIP code
    SUM(`LAHD FUNDED`) AS total_funding,  -- Total funding received across all projects per ZIP code
    AVG(`LAHD FUNDED`) AS average_funding_per_project,  -- Average funding per project per ZIP code
    SUM(LEVERAGE) AS total_leverage,  -- Total leverage funds across all projects per ZIP code
    AVG(LEVERAGE) AS average_leverage_per_project,  -- Average leverage per project per ZIP code
    SUM(TDC) AS total_development_cost,  -- Total development cost across all projects per ZIP code
    AVG(TDC) AS average_development_cost_per_project,  -- Average development cost per project per ZIP code
    SUM(JOBS) AS total_jobs_created,  -- Total number of jobs created across all projects per ZIP code
    AVG(JOBS) AS average_jobs_per_project  -- Average number of jobs per project per ZIP code
FROM
    housing_data
GROUP BY
    ZIP_CODE
ORDER BY
    total_projects DESC;  -- Order results by total number of projects in descending order
"""

# Execute the query and fetch results
housing_summary = pd.read_sql_query(housing_query, conn)  # Run the SQL query and load the result into a DataFrame
print(housing_summary.head())  # Print the first few rows of the summary DataFrame

"""# d) Join the files using SQL by ZIP code: [2]
# i. Ensure that the ZIP codes are in compatible formats and lengths
# ii. For each ZIP, get the predictor variable from the housing projects file, and potential predictedvariables from the health inspections file **bold text**
"""

# Query to join housing and inspection data
join_query = """
SELECT
    h.ZIP_CODE,
    h.total_projects,
    h.total_units,
    h.average_units_per_project,
    h.total_funding,
    h.average_funding_per_project,
    h.total_leverage,
    h.average_leverage_per_project,
    h.total_development_cost,
    h.average_development_cost_per_project,
    h.total_jobs_created,
    h.average_jobs_per_project,
    i.total_inspections,
    i.average_score,
    i.highest_score,
    i.lowest_score,
    i.grade_A_count,
    i.grade_B_count,
    i.grade_C_count,
    i.ungraded_count
FROM
    (SELECT
        ZIP_CODE,
        COUNT(*) AS total_projects,  -- Total number of housing projects per ZIP code
        SUM(`SITE UNITS`) AS total_units,  -- Total number of units across all projects per ZIP code
        AVG(`SITE UNITS`) AS average_units_per_project,  -- Average number of units per project per ZIP code
        SUM(`LAHD FUNDED`) AS total_funding,  -- Total funding received across all projects per ZIP code
        AVG(`LAHD FUNDED`) AS average_funding_per_project,  -- Average funding per project per ZIP code
        SUM(LEVERAGE) AS total_leverage,  -- Total leverage funds across all projects per ZIP code
        AVG(LEVERAGE) AS average_leverage_per_project,  -- Average leverage per project per ZIP code
        SUM(TDC) AS total_development_cost,  -- Total development cost across all projects per ZIP code
        AVG(TDC) AS average_development_cost_per_project,  -- Average development cost per project per ZIP code
        SUM(JOBS) AS total_jobs_created,  -- Total number of jobs created across all projects per ZIP code
        AVG(JOBS) AS average_jobs_per_project  -- Average number of jobs per project per ZIP code
     FROM
        housing_data
     GROUP BY
        ZIP_CODE) h
LEFT JOIN
    (SELECT
        facility_zip AS ZIP_CODE,
        COUNT(*) AS total_inspections,  -- Total number of inspections per ZIP code
        AVG(score) AS average_score,  -- Average inspection score per ZIP code
        MAX(score) AS highest_score,  -- Highest inspection score per ZIP code
        MIN(score) AS lowest_score,  -- Lowest inspection score per ZIP code
        COUNT(CASE WHEN grade = 'A' THEN 1 END) AS grade_A_count,  -- Count of 'A' grades per ZIP code
        COUNT(CASE WHEN grade = 'B' THEN 1 END) AS grade_B_count,  -- Count of 'B' grades per ZIP code
        COUNT(CASE WHEN grade = 'C' THEN 1 END) AS grade_C_count,  -- Count of 'C' grades per ZIP code
        COUNT(CASE WHEN grade = ' ' OR grade IS NULL THEN 1 END) AS ungraded_count  -- Count of ungraded inspections per ZIP code
     FROM
        inspection_data
     GROUP BY
        facility_zip) i
ON h.ZIP_CODE = i.ZIP_CODE  -- Perform a LEFT JOIN on the ZIP_CODE column
ORDER BY
    h.total_projects DESC;  -- Order results by the total number of projects in descending order
"""

# Execute the query and fetch the results
joined_data = pd.read_sql_query(join_query, conn)  # Run the SQL query and load the result into a DataFrame
print(joined_data.head())  # Print the first few rows of the joined DataFrame
#[8][5]

# Close the connection to the SQLite database
conn.close()  # Properly close the connection to the 'data_summary.db' database to free up resources

"""# e) Formulate and test the hypothesis: [2]
# i. Formulate a reasonable alternative hypothesis
#ii. Formulate a null hypothesis
#iii. Select an appropriate test and significance level
# iv. Perform the test and decide if the null hypothesis should be rejected and alternativehypothesis should be accepted **bold text**
"""

# Print the column names of the joined_data DataFrame
print(joined_data.columns)  # Display the names of all columns in the joined_data DataFrame

# the data types of joined_data

print(joined_data.dtypes)

# Convert the 'ZIP_CODE' column to numeric values, handling errors and missing values
joined_data['ZIP_CODE'] = pd.to_numeric(joined_data['ZIP_CODE'], errors='coerce')  # Convert 'ZIP_CODE' to numeric, setting invalid parsing to NaN
joined_data['ZIP_CODE'] = joined_data['ZIP_CODE'].fillna(0).astype(int)  # Fill NaNs with 0 and convert the column to integer type
#[8]

print(joined_data.dtypes)

# Print the count of missing values for each column in the joined_data DataFrame
print(joined_data.isnull().sum())  # Display the number of missing (NaN) values for each column in the joined_data DataFrame

"""Possible Reasons for Missing Values After the SQL Join
Even after cleaning missing values in the inspection data, missing values might still appear in the joined dataset due to several reasons:

**No Matching ZIP Code:**
ZIP codes present in the housing data but absent in the inspection data (or vice versa) will result in NaN values for the columns from the non-matching dataset. For instance, if a ZIP code has affordable housing projects but no corresponding inspection records, the inspection-related fields will be NaN in the joined dataset.

**Different ZIP Code Formats:**
Discrepancies in ZIP code formatting between datasets (e.g., one dataset using 5-digit ZIP codes and another using 9-digit ZIP codes) can lead to unmatched rows, resulting in missing values.

**Incomplete Data in One Dataset:**
If one dataset lacks ZIP codes that are present in the other, missing values will occur in the joined dataset where data from one dataset does not have a corresponding match.
"""

# Print the number of rows in the joined_data DataFrame
print(len(joined_data))  # Display the total number of rows in the joined_data DataFrame

# Identify ZIP codes present in one dataset but missing from the other
missing_in_inspection = joined_data[joined_data['total_inspections'].isnull()]['ZIP_CODE'].unique()  # ZIP codes with no inspection data
missing_in_housing = joined_data[joined_data['total_projects'].isnull()]['ZIP_CODE'].unique()  # ZIP codes with no housing data

print("ZIP codes missing in inspection data:", missing_in_inspection)  # Print ZIP codes missing from inspection data
print("ZIP codes missing in housing data:", missing_in_housing)  # Print ZIP codes missing from housing data
#[5]

# Print the number of ZIP codes missing in inspection data
print(len(missing_in_inspection))  # Display the count of unique ZIP codes that are missing inspection data

"""Formulating Hypotheses



*   Null Hypothesis (H₀): There is no significant relationship between the total units in affordable housing projects and the average health inspection scores in a ZIP code.
*  Alternative Hypothesis (H₁): There is a significant relationship between the total units in affordable housing projects and the average health inspection scores in a ZIP code.




"""

# Fill missing values with zeros
filled_data = joined_data.fillna(0)  # Replace all missing (NaN) values in the joined_data DataFrame with zeros

# Proceed with correlation analysis
from scipy.stats import pearsonr  # Import the pearsonr function from scipy.stats for calculating Pearson correlation

x = filled_data['total_units']  # Define 'x' as the 'total_units' column
y = filled_data['average_score']  # Define 'y' as the 'average_score' column

# Calculate Pearson correlation coefficient and p-value
pearson_corr, pearson_p_value = pearsonr(x, y)  # Compute the Pearson correlation coefficient and p-value between 'total_units' and 'average_score'

print("Pearson Correlation Coefficient:", pearson_corr)  # Print the Pearson correlation coefficient
print("Pearson P-Value:", pearson_p_value)  # Print the p-value associated with the Pearson correlation coefficient
#[3]

"""Interpreting the Pearson Correlation Results

1. Pearson Correlation Coefficient (0.553)

Interpretation: The Pearson Correlation Coefficient of 0.553 signifies a moderate positive correlation between the total units in affordable housing projects and the average health inspection scores in a ZIP code. This implies that as the number of affordable housing units increases, the average health inspection score tends to increase as well. While this correlation is not perfect (a value of 1), it is strong enough to be noticeable.
2. Pearson P-Value (1.66e-12)

Interpretation: The P-Value of 1.66e-12 is extremely small, indicating that the observed correlation is statistically significant. This very low probability suggests that the correlation is unlikely to have occurred by chance, allowing us to confidently reject the null hypothesis that no correlation exists.


Conclusion Based on the Results

Reject the Null Hypothesis: Given the extremely low P-value, we reject the null hypothesis in favor of the alternative hypothesis.
Moderate Positive Correlation: The moderate positive correlation suggests that ZIP codes with more affordable housing units tend to have slightly higher health inspection scores for restaurants. However, this relationship is not very strong.

# **2. Open-ended: Find some interesting data from Indian government data portal https://www.data.gov.inand perform EDA, derive some insights using graphs, and perform a statistical test for an interestinghypothesis. No need to use multiple files for this question, unless you want to do the extra work for yourown learning. [4]**

Selected the Physical & Financial Progress of Pradhan Mantri Gram Sadak Yojna (PMGSY) as on date Dataset for EDA
"""

# Load the dataset from a CSV file into a DataFrame
df = pd.read_csv('PMGSY.csv')  # Read the CSV file named 'PMGSY.csv' into a DataFrame called 'df'

# Display the first few rows of the DataFrame to get an overview of the data
df.head()  # Show the first 5 rows of the DataFrame 'df'

# Find the number of unique values in the 'STATE_NAME' column
num_unique_states = df['STATE_NAME'].nunique()

# Print the result
print(f"Number of unique states: {num_unique_states}")

print(df.dtypes)

# Group by state and sum the number of roads sanctioned
roads_sanctioned_by_state = df.groupby('STATE_NAME')['NO_OF_ROAD_WORK_SANCTIONED'].sum()

# Print the result
print(roads_sanctioned_by_state)

# Create a bar plot to visualize the total number of roads sanctioned by each state
plt.figure(figsize=(12, 8))  # Set the figure size for the plot

# Generate a bar plot with states on the x-axis and number of roads sanctioned on the y-axis
sns.barplot(x=roads_sanctioned_by_state.index, y=roads_sanctioned_by_state.values, palette='viridis')  # Use the 'viridis' color palette

plt.title('Total Roads Sanctioned by State')  # Set the title of the plot
plt.xlabel('State Name')  # Label for the x-axis
plt.ylabel('Number of Roads Sanctioned')  # Label for the y-axis
plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees for better readability

plt.show()  # Display the plot
#[2]

# Group by state and sum the cost of works sanctioned
cost_sanctioned_by_state = df.groupby('STATE_NAME')['COST_OF_WORKS_SANCTIONED_LAKHS'].sum()

# Print the result
print(cost_sanctioned_by_state)

# Plot the cost of works sanctioned by state
plt.figure(figsize=(12, 8))  # Set the figure size for the plot

# Generate a bar plot with states on the x-axis and cost of works sanctioned on the y-axis
sns.barplot(x=cost_sanctioned_by_state.index, y=cost_sanctioned_by_state.values / 100, color='blue')  # Use a single color

plt.title('Total Cost of Works Sanctioned by State (in Crores)')  # Set the title of the plot
plt.xlabel('State Name')  # Label for the x-axis
plt.ylabel('Cost of Works Sanctioned (Crores)')  # Label for the y-axis
plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees for better readability

plt.show()  # Display the plot
#[2]

# Calculate the percentage of roads completed for each state
df['PERCENTAGE_COMPLETED'] = (df['NO_OF_ROAD_WORKS_COMPLETED'] / df['NO_OF_ROAD_WORK_SANCTIONED']) * 100

# Group by state and calculate the average percentage of roads completed
avg_completion_by_state = df.groupby('STATE_NAME')['PERCENTAGE_COMPLETED'].mean().sort_values(ascending=False)

# Print the result
print(avg_completion_by_state)

# Plot the average percentage of roads completed by state
plt.figure(figsize=(12, 8))  # Set the figure size for the plot

# Generate a bar plot with states on the x-axis and average percentage completed on the y-axis
sns.barplot(x=avg_completion_by_state.index, y=avg_completion_by_state.values, color='blue')  # Use a single color

plt.title('Average Percentage of Roads Completed by State')  # Set the title of the plot
plt.xlabel('State Name')  # Label for the x-axis
plt.ylabel('Average Percentage Completed')  # Label for the y-axis
plt.xticks(rotation=90)  # Rotate x-axis labels by 90 degrees for better readability

plt.show()  # Display the plot

# Plot the relationship between the cost sanctioned and the number of roads completed
plt.figure(figsize=(8, 6))  # Set the figure size for the plot

# Generate a scatter plot with cost of works sanctioned on the x-axis and number of roads completed on the y-axis
sns.scatterplot(x='COST_OF_WORKS_SANCTIONED_LAKHS', y='NO_OF_ROAD_WORKS_COMPLETED', data=df)

plt.title('Relationship between Cost Sanctioned and Roads Completed')  # Set the title of the plot
plt.xlabel('Cost of Works Sanctioned (Lakhs)')  # Label for the x-axis
plt.ylabel('Number of Roads Completed')  # Label for the y-axis
plt.show()  # Display the plot

# Calculate the correlation between cost sanctioned and roads completed
correlation = df['COST_OF_WORKS_SANCTIONED_LAKHS'].corr(df['NO_OF_ROAD_WORKS_COMPLETED'])
print("Correlation Coefficient:", correlation)

# Hypothesis testing:
# Null Hypothesis (H₀): There is no significant correlation between cost sanctioned and roads completed.
# Alternative Hypothesis (H₁): There is a significant positive correlation between cost sanctioned and roads completed.

# Perform Pearson correlation test
from scipy.stats import pearsonr

# Calculate Pearson correlation coefficient and p-value
corr, p_value = pearsonr(df['COST_OF_WORKS_SANCTIONED_LAKHS'], df['NO_OF_ROAD_WORKS_COMPLETED'])
print("Pearson Correlation Coefficient:", corr)


# Evaluate the significance of the correlation
if p_value < 0.05:
    print("Reject the null hypothesis. There is a significant correlation between cost sanctioned and roads completed.")
else:
    print("Fail to reject the null hypothesis. There is no significant correlation between cost sanctioned and roads completed.")

#[3][5][6]

"""# Null Hypothesis (H₀): There is no significant difference in the cost of works sanctioned between South Indian states and other states.

# Alternative Hypothesis (H₁): There is a significant difference in the cost of works sanctioned between South Indian states and other states.
"""

# Define South Indian states
south_indian_states = ['Andhra Pradesh', 'Karnataka', 'Kerala', 'Tamil Nadu', 'Telangana']

# Filter the dataframe for South Indian states
south_indian_df = df[df['STATE_NAME'].isin(south_indian_states)]

# Calculate the total cost of works sanctioned for South Indian states
total_cost_south_india = south_indian_df['COST_OF_WORKS_SANCTIONED_LAKHS'].sum()

# Calculate the total cost of works sanctioned for all states
total_cost_all_states = df['COST_OF_WORKS_SANCTIONED_LAKHS'].sum()

# Calculate the proportion of cost allocated to South Indian states
proportion_south_india = total_cost_south_india / total_cost_all_states

print(f"Total cost of works sanctioned for South Indian states: {total_cost_south_india}")
print(f"Total cost of works sanctioned for all states: {total_cost_all_states}")
print(f"Proportion of cost allocated to South Indian states: {proportion_south_india:.2f}")

# Separate data into South Indian states and other states
south_indian_df = df[df['STATE_NAME'].isin(south_indian_states)]
other_states_df = df[~df['STATE_NAME'].isin(south_indian_states)]

from scipy.stats import ttest_ind

# Perform t-test
t_stat, p_value = ttest_ind(
    south_indian_df['COST_OF_WORKS_SANCTIONED_LAKHS'].dropna(),
    other_states_df['COST_OF_WORKS_SANCTIONED_LAKHS'].dropna()
)

print("T-statistic:", t_stat)
print("P-value:", p_value)

if p_value < 0.05:
    print("Reject the null hypothesis. There is a significant difference in the allocation of cost of works sanctioned between South Indian states and other states.")
else:
    print("Fail to reject the null hypothesis. There is no significant difference in the allocation of cost of works sanctioned between South Indian states and other states.")

"""# **REFERENCES**

**Code without marking is written by me without any copying**
1. Pandas Documentation
2. Matplotlib Documentation
3. Scipy Documentation
4. Seaborn Documentation
5. Discussion with Nafrin PE (23M0138)
6. ChatGpt
7. Extracting ZIP codes using regex and Pandas - ChatGPT prompt and personal knowledge (based on standard regex practices and Pandas DataFrame operations).
8. ChatGPT prompt: How to join two SQL tables and aggregate data
"""