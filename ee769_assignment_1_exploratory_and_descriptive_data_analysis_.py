# -*- coding: utf-8 -*-
"""EE769  Assignment 1: Exploratory and Descriptive Data Analysis .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MbKmv1T_DJhaLEymimp60jRfR-xvTgN8
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
inspection_data = pd.read_csv('restaurant_and_market_health_inspections.csv')
housing_data = pd.read_csv('hcidla_affordable_housing_projects.csv')



# Check Data Types
print(inspection_data.dtypes)

inspection_data['facility_zip'] = inspection_data['facility_zip'].astype(str)  # Ensure it's a string
inspection_data['facility_zip'] = inspection_data['facility_zip'].str.replace('-', '', regex=False)  # Remove hyphens
inspection_data['facility_zip'] = inspection_data['facility_zip'].str[:5]  # Keep only the first 5 digits

inspection_data['activity_date'] = pd.to_datetime(inspection_data['activity_date'])
inspection_data['facility_zip'] = inspection_data['facility_zip'].astype(int)

print(inspection_data.dtypes)

print(inspection_data['facility_zip'].nunique())
print(inspection_data['facility_name'].nunique())
print(inspection_data['grade'].unique())

inspection_data['grade'].replace(' ', 'Ungraded', inplace=True)
inspection_data['grade'].fillna('Ungraded', inplace=True)
print(inspection_data['grade'].unique())

plt.figure(figsize=(8, 6))
sns.countplot(x='grade', data=inspection_data, palette='pastel', order=['A', 'B', 'C', 'Ungraded'])
plt.title('Distribution of Health Inspection Grades')
plt.xlabel('Grade')
plt.ylabel('Number of Inspections')
plt.show()

print(inspection_data['score'].unique())

zip_code_counts = inspection_data['facility_zip'].value_counts()
print(zip_code_counts.head(10))  # Display the top 10 ZIP codes with the most inspections

plt.figure(figsize=(12, 8))
sns.barplot(x=zip_code_counts.index[:20], y=zip_code_counts.values[:20], palette='viridis')
plt.title('Top 20 ZIP Codes by Number of Inspections')
plt.xlabel('ZIP Code')
plt.ylabel('Number of Inspections')
plt.xticks(rotation=90)
plt.show()

avg_scores_by_zip = inspection_data.groupby('facility_zip')['score'].mean().sort_values(ascending=False)
print(avg_scores_by_zip.head(30))  # Display the top 10 ZIP codes with the highest average scores

plt.figure(figsize=(12, 8))
sns.barplot(x=avg_scores_by_zip.index[:20], y=avg_scores_by_zip.values[:20], palette='viridis')
plt.title('Top 20 ZIP Codes by Average Inspection Score')
plt.xlabel('ZIP Code')
plt.ylabel('Average Score')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(inspection_data['score'], bins=20, kde=True, color='blue')
plt.title('Distribution of Health Inspection Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')
plt.show()

print(inspection_data.isnull().sum())

inspection_data.dropna(subset=['pe_description', 'program_element_pe','program_name','program_status','record_id' ], inplace=True)

print(inspection_data.isnull().sum())

# Create a DataFrame to compare the two columns
comparison_df = inspection_data[['program_name', 'facility_name']].drop_duplicates()
comparison_df['match'] = comparison_df['program_name'].str.strip().str.lower() == comparison_df['facility_name'].str.strip().str.lower()

# Display rows where there is a mismatch
mismatches = comparison_df[~comparison_df['match']]
print(f"Number of mismatches: {len(mismatches)}")
print(mismatches.head(10))

"""# **Housing data EDA**"""

print(housing_data.dtypes)

housing_data['DATE FUNDED'] = pd.to_datetime(housing_data['DATE FUNDED'], errors='coerce')
housing_data['IN-SERVICE DATE'] = pd.to_datetime(housing_data['IN-SERVICE DATE'], errors='coerce')

print(housing_data.dtypes)

print(housing_data.isnull().sum())

housing_data.dropna(subset=['DEVELOPMENT STAGE', 'DATE FUNDED'], inplace=True)
housing_data['CONSTRUCTION TYPE'].fillna('Unknown', inplace=True)
housing_data['SITE COMMUNITY'].fillna('Unknown', inplace=True)
housing_data['HOUSING TYPE'].fillna('Unknown', inplace=True)
housing_data['IN-SERVICE DATE'].fillna(pd.Timestamp('1900-01-01'), inplace=True)  # Example date
housing_data['DEVELOPER'].fillna('Unknown', inplace=True)
housing_data['MANAGEMENT COMPANY'].fillna('Unknown', inplace=True)
housing_data['JOBS'].fillna('Unknown', inplace=True)
housing_data.drop(columns=['CONTACT PHONE', 'CONTRACT NUMBERS'], inplace=True)

print(housing_data.isnull().sum())

import re

# Function to extract ZIP code from an address
def extract_zip_code(address):
    match = re.search(r'\b\d{5}\b', address)
    return match.group(0) if match else None

# Apply the function to the SITE ADDRESS column
housing_data['ZIP_CODE'] = housing_data['SITE ADDRESS'].apply(extract_zip_code)

housing_data.to_csv('housing_data.csv', index=False)

# Check the first few rows to verify
print(housing_data[['SITE ADDRESS', 'ZIP_CODE']].head())

print(housing_data.isnull().sum())

print(housing_data['ZIP_CODE'].isnull().sum())
print(housing_data['ZIP_CODE'].unique())

zip_code_projects = housing_data['ZIP_CODE'].value_counts()

plt.figure(figsize=(12, 8))
sns.barplot(x=zip_code_projects.index[:20], y=zip_code_projects.values[:20], palette='viridis')
plt.title('Top 20 ZIP Codes by Number of Affordable Housing Projects')
plt.xlabel('ZIP Code')
plt.ylabel('Number of Projects')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12, 8))
sns.countplot(x='HOUSING TYPE', data=housing_data, palette='pastel')
plt.title('Distribution of Housing Types')
plt.xlabel('Housing Type')
plt.ylabel('Number of Projects')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(housing_data['PROJECT TOTAL UNITS'], bins=20, kde=True, color='green')
plt.title('Distribution of Project Sizes (Total Units)')
plt.xlabel('Total Units')
plt.ylabel('Frequency')
plt.show()

print(housing_data.columns)

print(housing_data['SITE ADDRESS'].isnull().sum())
print(inspection_data['facility_zip'].isnull().sum())

"""# c) Summarize each file by ZIP code using SQL: [2]
# i. Ensure the right type of summarization (sum, mean, max etc.) for the other variables **bold text**
"""

import sqlite3
import pandas as pd

conn = sqlite3.connect('data_summary.db')

# Load inspection data into SQLite
#inspection_data = pd.read_csv('restaurant_and_market_health_inspections.csv')
inspection_data.to_sql('inspection_data', conn, if_exists='replace', index=False)

# Load housing data into SQLite
#housing_data = pd.read_csv('hcidla_affordable_housing_projects.csv')
housing_data.to_sql('housing_data', conn, if_exists='replace', index=False)

schema_query = "PRAGMA table_info(housing_data);"
schema = pd.read_sql_query(schema_query, conn)
print(schema)

# Query for inspection data summary
inspection_query = """
SELECT
    facility_zip AS ZIP_CODE,
    COUNT(*) AS total_inspections,
    AVG(score) AS average_score,
    MAX(score) AS highest_score,
    MIN(score) AS lowest_score,
    COUNT(CASE WHEN grade = 'A' THEN 1 END) AS grade_A_count,
    COUNT(CASE WHEN grade = 'B' THEN 1 END) AS grade_B_count,
    COUNT(CASE WHEN grade = 'C' THEN 1 END) AS grade_C_count,
    COUNT(CASE WHEN grade = ' ' OR grade IS NULL THEN 1 END) AS ungraded_count
FROM
    inspection_data
GROUP BY
    facility_zip
ORDER BY
    total_inspections DESC;
"""

# Execute the query and fetch results
inspection_summary = pd.read_sql_query(inspection_query, conn)
print(inspection_summary.head())

# Query for housing data summary
housing_query = """
SELECT
    ZIP_CODE,
    COUNT(*) AS total_projects,
    SUM(`SITE UNITS`) AS total_units,
    AVG(`SITE UNITS`) AS average_units_per_project,
    SUM(`LAHD FUNDED`) AS total_funding,
    AVG(`LAHD FUNDED`) AS average_funding_per_project,
    SUM(LEVERAGE) AS total_leverage,
    AVG(LEVERAGE) AS average_leverage_per_project,
    SUM(TDC) AS total_development_cost,
    AVG(TDC) AS average_development_cost_per_project,
    SUM(JOBS) AS total_jobs_created,
    AVG(JOBS) AS average_jobs_per_project
FROM
    housing_data
GROUP BY
    ZIP_CODE
ORDER BY
    total_projects DESC;
"""

# Execute the query and fetch results
housing_summary = pd.read_sql_query(housing_query, conn)
print(housing_summary.head())

"""# d) Join the files using SQL by ZIP code: [2]
# i. Ensure that the ZIP codes are in compatible formats and lengths
# ii. For each ZIP, get the predictor variable from the housing projects file, and potential predictedvariables from the health inspections file **bold text**
"""

join_query = """
SELECT
    h.ZIP_CODE,
    h.total_projects,
    h.total_units,
    h.average_units_per_project,
    h.total_funding,
    h.average_funding_per_project,
    h.total_leverage,
    h.average_leverage_per_project,
    h.total_development_cost,
    h.average_development_cost_per_project,
    h.total_jobs_created,
    h.average_jobs_per_project,
    i.total_inspections,
    i.average_score,
    i.highest_score,
    i.lowest_score,
    i.grade_A_count,
    i.grade_B_count,
    i.grade_C_count,
    i.ungraded_count
FROM
    (SELECT
        ZIP_CODE,
        COUNT(*) AS total_projects,
        SUM(`SITE UNITS`) AS total_units,
        AVG(`SITE UNITS`) AS average_units_per_project,
        SUM(`LAHD FUNDED`) AS total_funding,
        AVG(`LAHD FUNDED`) AS average_funding_per_project,
        SUM(LEVERAGE) AS total_leverage,
        AVG(LEVERAGE) AS average_leverage_per_project,
        SUM(TDC) AS total_development_cost,
        AVG(TDC) AS average_development_cost_per_project,
        SUM(JOBS) AS total_jobs_created,
        AVG(JOBS) AS average_jobs_per_project
     FROM
        housing_data
     GROUP BY
        ZIP_CODE) h
LEFT JOIN
    (SELECT
        facility_zip AS ZIP_CODE,
        COUNT(*) AS total_inspections,
        AVG(score) AS average_score,
        MAX(score) AS highest_score,
        MIN(score) AS lowest_score,
        COUNT(CASE WHEN grade = 'A' THEN 1 END) AS grade_A_count,
        COUNT(CASE WHEN grade = 'B' THEN 1 END) AS grade_B_count,
        COUNT(CASE WHEN grade = 'C' THEN 1 END) AS grade_C_count,
        COUNT(CASE WHEN grade = ' ' OR grade IS NULL THEN 1 END) AS ungraded_count
     FROM
        inspection_data
     GROUP BY
        facility_zip) i
ON h.ZIP_CODE = i.ZIP_CODE
ORDER BY
    h.total_projects DESC;
"""

# Execute the query and fetch the results
joined_data = pd.read_sql_query(join_query, conn)
print(joined_data.head())

conn.close()

"""# e) Formulate and test the hypothesis: [2]
# i. Formulate a reasonable alternative hypothesis
#ii. Formulate a null hypothesis
#iii. Select an appropriate test and significance level
# iv. Perform the test and decide if the null hypothesis should be rejected and alternativehypothesis should be accepted **bold text**
"""

print(joined_data.columns)

# prompt: show me the data types of joined_data

print(joined_data.dtypes)

# prompt: convert zip_code to integer

joined_data['ZIP_CODE'] = pd.to_numeric(joined_data['ZIP_CODE'], errors='coerce').fillna(0).astype(int)

print(joined_data.dtypes)

# prompt: find missing values

print(joined_data.isnull().sum())

print(len(joined_data))

# Identify ZIP codes present in one dataset but missing from the other
missing_in_inspection = joined_data[joined_data['total_inspections'].isnull()]['ZIP_CODE'].unique()
missing_in_housing = joined_data[joined_data['total_projects'].isnull()]['ZIP_CODE'].unique()

print("ZIP codes missing in inspection data:", missing_in_inspection)
print("ZIP codes missing in housing data:", missing_in_housing)

print(len(missing_in_inspection))

# Fill missing values with zeros
filled_data = joined_data.fillna(0)

# Proceed with correlation analysis
from scipy.stats import pearsonr

x = filled_data['total_units']
y = filled_data['average_score']

pearson_corr, pearson_p_value = pearsonr(x, y)
print("Pearson Correlation Coefficient:", pearson_corr)
print("Pearson P-Value:", pearson_p_value)



"""# **2. Open-ended: Find some interesting data from Indian government data portal https://www.data.gov.inand perform EDA, derive some insights using graphs, and perform a statistical test for an interestinghypothesis. No need to use multiple files for this question, unless you want to do the extra work for yourown learning. [4]**

Selected the Physical & Financial Progress of Pradhan Mantri Gram Sadak Yojna (PMGSY) as on date Dataset for EDA
"""

df = pd.read_csv('PMGSY.csv')

df.head()

# Find the number of unique values in the 'STATE_NAME' column
num_unique_states = df['STATE_NAME'].nunique()

# Print the result
print(f"Number of unique states: {num_unique_states}")

print(df.dtypes)

# Group by state and sum the number of roads sanctioned
roads_sanctioned_by_state = df.groupby('STATE_NAME')['NO_OF_ROAD_WORK_SANCTIONED'].sum()

# Print the result
print(roads_sanctioned_by_state)

plt.figure(figsize=(12, 8))
sns.barplot(x=roads_sanctioned_by_state.index, y=roads_sanctioned_by_state.values, palette='viridis')
plt.title('Total Roads Sanctioned by State')
plt.xlabel('State Name')
plt.ylabel('Number of Roads Sanctioned')
plt.xticks(rotation=90)
plt.show()

# Group by state and sum the cost of works sanctioned
cost_sanctioned_by_state = df.groupby('STATE_NAME')['COST_OF_WORKS_SANCTIONED_LAKHS'].sum()

# Print the result
print(cost_sanctioned_by_state)

# Plot the cost of works sanctioned by state
plt.figure(figsize=(12, 8))
sns.barplot(x=cost_sanctioned_by_state.index, y=cost_sanctioned_by_state.values/100, palette='viridis')
plt.title('Total Cost of Works Sanctioned by State (in Crores)')
plt.xlabel('State Name')
plt.ylabel('Cost of Works Sanctioned (Cores)')
plt.xticks(rotation=90)
plt.show()

# prompt: do some interesing analysis on df dataframe

# Calculate the percentage of roads completed for each state
df['PERCENTAGE_COMPLETED'] = (df['NO_OF_ROAD_WORKS_COMPLETED'] / df['NO_OF_ROAD_WORK_SANCTIONED']) * 100

# Group by state and calculate the average percentage of roads completed
avg_completion_by_state = df.groupby('STATE_NAME')['PERCENTAGE_COMPLETED'].mean().sort_values(ascending=False)

# Print the result
print(avg_completion_by_state)

# Plot the average percentage of roads completed by state
plt.figure(figsize=(12, 8))
sns.barplot(x=avg_completion_by_state.index, y=avg_completion_by_state.values, palette='viridis')
plt.title('Average Percentage of Roads Completed by State')
plt.xlabel('State Name')
plt.ylabel('Average Percentage Completed')
plt.xticks(rotation=90)
plt.show()

# Analyze the relationship between cost sanctioned and roads completed
plt.figure(figsize=(8, 6))
sns.scatterplot(x='COST_OF_WORKS_SANCTIONED_LAKHS', y='NO_OF_ROAD_WORKS_COMPLETED', data=df)
plt.title('Relationship between Cost Sanctioned and Roads Completed')
plt.xlabel('Cost of Works Sanctioned (Lakhs)')
plt.ylabel('Number of Roads Completed')
plt.show()

# Calculate the correlation between cost sanctioned and roads completed
correlation = df['COST_OF_WORKS_SANCTIONED_LAKHS'].corr(df['NO_OF_ROAD_WORKS_COMPLETED'])


# Hypothesis testing:
# Null Hypothesis: There is no significant correlation between cost sanctioned and roads completed.
# Alternative Hypothesis: There is a significant positive correlation between cost sanctioned and roads completed.

# Test: Pearson correlation test
# Significance level: 0.05

from scipy.stats import pearsonr

corr, p_value = pearsonr(df['COST_OF_WORKS_SANCTIONED_LAKHS'], df['NO_OF_ROAD_WORKS_COMPLETED'])
print("Pearson Correlation Coefficient:", corr)
print("P-value:", p_value)

if p_value < 0.05:
  print("Reject the null hypothesis. There is a significant correlation between cost sanctioned and roads completed.")
else:
  print("Fail to reject the null hypothesis. There is no significant correlation between cost sanctioned and roads completed.")

"""# Null Hypothesis (H₀): There is no significant difference in the cost of works sanctioned between South Indian states and other states.

# Alternative Hypothesis (H₁): There is a significant difference in the cost of works sanctioned between South Indian states and other states.
"""

# Define South Indian states
south_indian_states = ['Andhra Pradesh', 'Karnataka', 'Kerala', 'Tamil Nadu', 'Telangana']

# Filter the dataframe for South Indian states
south_indian_df = df[df['STATE_NAME'].isin(south_indian_states)]

# Calculate the total cost of works sanctioned for South Indian states
total_cost_south_india = south_indian_df['COST_OF_WORKS_SANCTIONED_LAKHS'].sum()

# Calculate the total cost of works sanctioned for all states
total_cost_all_states = df['COST_OF_WORKS_SANCTIONED_LAKHS'].sum()

# Calculate the proportion of cost allocated to South Indian states
proportion_south_india = total_cost_south_india / total_cost_all_states

print(f"Total cost of works sanctioned for South Indian states: {total_cost_south_india}")
print(f"Total cost of works sanctioned for all states: {total_cost_all_states}")
print(f"Proportion of cost allocated to South Indian states: {proportion_south_india:.2f}")

# Separate data into South Indian states and other states
south_indian_df = df[df['STATE_NAME'].isin(south_indian_states)]
other_states_df = df[~df['STATE_NAME'].isin(south_indian_states)]

from scipy.stats import ttest_ind

# Perform t-test
t_stat, p_value = ttest_ind(
    south_indian_df['COST_OF_WORKS_SANCTIONED_LAKHS'].dropna(),
    other_states_df['COST_OF_WORKS_SANCTIONED_LAKHS'].dropna()
)

print("T-statistic:", t_stat)
print("P-value:", p_value)

if p_value < 0.05:
    print("Reject the null hypothesis. There is a significant difference in the allocation of cost of works sanctioned between South Indian states and other states.")
else:
    print("Fail to reject the null hypothesis. There is no significant difference in the allocation of cost of works sanctioned between South Indian states and other states.")

plt.figure(figsize=(10, 6))
sns.violinplot(x='Region', y='COST_OF_WORKS_SANCTIONED_LAKHS', data=combined_df, palette=['#1f77b4', '#ff7f0e'])
plt.title('Distribution of Cost of Works Sanctioned: South Indian States vs Other States')
plt.xlabel('Region')
plt.ylabel('Cost of Works Sanctioned (Lakhs)')
plt.show()

